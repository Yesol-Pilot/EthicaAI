# EthicaAI â€” Twitter/X í™ë³´ ì“°ë ˆë“œ ì´ˆì•ˆ

> ì‹¤í—˜ ê²°ê³¼ í™•ì • í›„ ìˆ˜ì¹˜ ì—…ë°ì´íŠ¸ í•„ìš”. ì•„ë˜ëŠ” ì´ˆì•ˆ.

---

## Thread (ì˜ì–´)

### 1/6 ğŸ§µ
Can AI agents learn *morality* instead of just maximizing rewards?

We computationally verified Amartya Sen's "Meta-Ranking" theoryâ€”
preferences over preferencesâ€”in a 100-agent social dilemma.

3 surprising findings â†“

### 2/6 â€” Finding 1: Dynamic > Static
Injecting fixed social values (SVO) into agents does nothing meaningful (p=0.64).

But giving agents the ability to *dynamically switch* between self-interest and morality?

That changes everything. (p=0.0023, HAC Robust SE)

### 3/6 â€” Finding 2: Role Specialization
We expected prosocial agents to cooperate more.

Instead, they *specialize*: some become cleaners ğŸ§¹, others become harvesters ğŸŒ¾

This emergent division of labor is structurally more efficient than uniform cooperation.

[Fig 9 â€” Role Specialization ì²¨ë¶€]

### 4/6 â€” Finding 3: Only "Situational Commitment" Survives
Full altruists? They go extinct. Purely selfish? Also die out.

The only Evolutionarily Stable Strategy:
"Help when I can afford it, prioritize survival when I can't."

Rational morality > absolute morality.

### 5/6 â€” Why It Matters for AI Alignment
Current approaches to AI alignment either:
- Hardcode rules (brittle) ğŸ“
- Learn from human feedback (costly) ğŸ·ï¸

Meta-Ranking offers a middle path:
Let agents *learn* when to be moralâ€”not just *what* morality is.

### 6/6 â€” Links
ğŸ“„ Paper (Zenodo): [DOI ë§í¬]
ğŸ’» Code: https://github.com/Yesol-Pilot/EthicaAI
ğŸ§ª JAX + PPO, reproducible in ~35 min on RTX 4070

Looking for cs.MA endorser for arXiv! DM if interested ğŸ™

---

## í•´ì‹œíƒœê·¸
#MARL #AIAlignment #GameTheory #ReinforcementLearning #AmartyaSen
#MultiAgentSystems #AIEthics #NeurIPS2026

---

## í•œêµ­ì–´ (ì„ íƒ)

### 1/4
AI ì—ì´ì „íŠ¸ê°€ ë³´ìƒ ìµœëŒ€í™”ê°€ ì•„ë‹ˆë¼ 'ë„ë•'ì„ ë°°ìš¸ ìˆ˜ ìˆì„ê¹Œ?

ë…¸ë²¨ê²½ì œí•™ìƒ ìˆ˜ìƒì ì•„ë§ˆë¥´í‹°ì•„ ì„¼ì˜ "ë©”íƒ€ë­í‚¹" ì´ë¡ ì„ 
100-ì—ì´ì „íŠ¸ ì‚¬íšŒì  ë”œë ˆë§ˆì—ì„œ ê³„ì‚°ì ìœ¼ë¡œ ê²€ì¦í–ˆìŠµë‹ˆë‹¤.

### 2/4
í•µì‹¬ ë°œê²¬:
1ï¸âƒ£ ê³ ì •ëœ ì‚¬íšŒì  ê°€ì¹˜ ì£¼ì…ì€ íš¨ê³¼ ì—†ìŒ
2ï¸âƒ£ ë™ì  ë„ë• ì „í™˜(Î»_t)ë§Œì´ ì§‘ë‹¨ ë³µì§€ë¥¼ ìœ ì˜í•˜ê²Œ í–¥ìƒ
3ï¸âƒ£ "ìƒí™©ì  í—Œì‹ "ë§Œì´ ì§„í™”ì ìœ¼ë¡œ ì•ˆì • â€” ì ˆëŒ€ì  ì´íƒ€ì£¼ì˜ëŠ” ë©¸ì¢…

### 3/4
AI ì •ë ¬(alignment)ì— ëŒ€í•œ ì‹œì‚¬ì :
ê·œì¹™ì„ í•˜ë“œì½”ë”©í•˜ì§€ ë§ˆì„¸ìš”. 
ì—ì´ì „íŠ¸ê°€ "ì–¸ì œ ë„ë•ì ì´ì–´ì•¼ í•˜ëŠ”ì§€"ë¥¼ í•™ìŠµí•˜ê²Œ í•˜ì„¸ìš”.

### 4/4
ğŸ“„ ë…¼ë¬¸: Zenodo (DOI)
ğŸ’» ì½”ë“œ: GitHub
ë…ë¦½ ì—°êµ¬ìë¡œì„œ ì²« ë…¼ë¬¸ì…ë‹ˆë‹¤. í”¼ë“œë°± í™˜ì˜í•©ë‹ˆë‹¤!
