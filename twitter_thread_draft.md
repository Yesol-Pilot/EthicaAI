# EthicaAI Twitter/X Thread Draft ğŸ§µ

> ìƒˆ ì—°êµ¬: AI ì—ì´ì „íŠ¸ëŠ” *ì–¸ì œ* ë„ë•ì ì´ì–´ì•¼ í•˜ëŠ”ê°€?

---

## Thread (10 tweets)

### 1/10 ğŸ¯
ğŸ§µ NEW PAPER: "When Should AI Agents Be Moral?"

We tested Amartya Sen's meta-ranking theory in Multi-Agent RL across 4 environments, 7 SVO types, up to 1000 agents.

30 figures. 560+ experiments. One key insight: Static morality fails. Dynamic does not.

#AIAlignment #MARL #NeurIPS

### 2/10 ğŸ“Š
Finding 1: Dynamic meta-ranking (Î»_t) significantly improves collective welfare (p=0.0003).

Static value injection? Fails completely.

The key: agents must learn *when* to be moral, not encode fixed values.

[Fig 1: Learning Curves]

### 3/10 ğŸ”„
Finding 2: Only "Situational Commitment" survives evolution.

In replicator dynamics over 200 generations, meta-ranking converges to ~12% of the population â€” regardless of starting conditions.

A "Moral Minority" is sufficient for cooperation.

[Fig 17: Evolutionary Dynamics]

### 4/10 ğŸ§¬
Finding 3: The "Rational Fool" is real.

Individualist SVO (Î¸=15Â°) â€” not pure altruists â€” best matches human Public Goods Game data (Wasserstein Distance = 0.053).

Sen was right: bounded self-interest, not sainthood, is human nature.

[Fig 19: Human Comparison]

### 5/10 ğŸŒ
NEW: Full Sweep across 4 environments (Cleanup, IPD, PGG, Harvest).

560 runs confirm: Meta-ranking's strongest effect is in common-pool resources.

Harvest ATE(Coop) = +0.506 â€” for *selfish* agents!
Crisis-driven Î» suppression prevents over-harvesting.

[Fig 24: Full Sweep Heatmap]

### 6/10 ğŸ¤
NEW: Mixed-SVO Populations reveal a tipping point.

At ~30% prosocial fraction, collective welfare jumps nonlinearly.

PGG: Max welfare improvement Î”W = +10,080 â€” superlinear scaling.

You don't need everyone to be moral. Just enough.

[Fig 25: Tipping Point]

### 7/10 ğŸ“¡
NEW: Communication channels boost cooperation +5.8%.

But here's the twist: message truthfulness converges to 98%.

Under meta-ranking, *honesty is evolutionarily favored*. Cheap talk becomes trustworthy talk.

[Fig 27-28: Communication]

### 8/10 ğŸ”„
NEW: Continuous action spaces.

When agents can choose any contribution âˆˆ [0, 100%], meta-ranking ATE remains â‰ˆ +0.20.

Beta-distribution policies show smooth Î» adaptation instead of binary switching. The mechanism generalizes beyond discrete decisions.

[Fig 29-30: Continuous PGG]

### 9/10 ğŸ›ï¸
Three implications for AI Alignment:

1. Don't hardcode morality â†’ Learn *when* to commit
2. A moral minority (~12%) is an ESS â†’ Universal morality is unnecessary
3. Bounded self-interest (Î¸=15Â°) is human nature â†’ Design for that

### 10/10 ğŸ”—
ğŸ“„ Paper: [Coming to arXiv]
ğŸŒ Dashboard: https://ethicaai.vercel.app
ğŸ’» Code: https://github.com/Yesol-Pilot/EthicaAI

30 figures | 4 environments | 1000 agents | 560+ experiments

EthicaAI â€” Because the question isn't *whether* AI should be moral, but *when*.

#AIEthics #ReinforcementLearning #GameTheory

---

## Key Hashtags
- #AIAlignment
- #MARL
- #NeurIPS2026
- #AIEthics
- #ReinforcementLearning
- #GameTheory
- #ComputationalSocialScience
