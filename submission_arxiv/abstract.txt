Integrating AI agents into human society requires resolving the fundamental conflict between self-interest and social values. This study formalizes Amartya Sen's Meta-Ranking theory—preferences over preferences—within a Multi-Agent Reinforcement Learning (MARL) framework. We simulate agents with 7 Social Value Orientations (SVOs) across 4 environments (Cleanup, IPD, PGG, Harvest) at scales up to 1,000 agents.

Five key findings: (1) Dynamic meta-ranking significantly improves collective welfare (p=0.0003) while static SVO injection fails; (2) Agents exhibit spontaneous role specialization; (3) Only 'Situational Commitment' survives as an ESS, converging to ~12% of the population; (4) Individualist SVO (theta=15°) best matches human behavioral data (WD=0.053); (5) Full factorial decomposition shows SVO rotation accounts for 86% of the effect.

Extended experiments demonstrate generalizability across 4 environments (560 runs), tipping points in mixed-motive populations (~30%), communication-enhanced cooperation (+5.8%), and continuous action space robustness (ATE≈+0.20). These results provide design principles for AI systems that can promote the common good without becoming 'suckers' — computationally realizing morality as an Evolutionarily Stable Strategy.