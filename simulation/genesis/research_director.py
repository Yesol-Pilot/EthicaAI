"""
ResearchDirector: EthicaAI ììœ¨ ì—°êµ¬ì†Œì˜ ìµœìƒìœ„ ì˜ì‚¬ê²°ì • ì—ì´ì „íŠ¸.

ì—­í• :
- ì—°êµ¬ ì˜ì œ(Research Agenda) ê´€ë¦¬
- ì„±ê³µ ì‹œ í›„ì† ì—°êµ¬ ìë™ ìƒì„±
- ì‹¤íŒ¨ ì‹œ ëŒ€ì•ˆ ì—°êµ¬ë¡œ ì „í™˜
- LLMì„ í™œìš©í•œ ìƒˆ ì—°êµ¬ ì§ˆë¬¸ ìë™ ìƒì„±
- v2.0: íŠ¸ë¦¬ íƒìƒ‰ ëª¨ë“œ ì§€ì› (AgenticTreeSearch ì—°ë™)
"""

import os
import json
from datetime import datetime

try:
    import google.generativeai as genai
except ImportError:
    genai = None


class ResearchDirector:
    """
    5ë²ˆì§¸ ì—ì´ì „íŠ¸: ì—°êµ¬ ì˜ì œë¥¼ ê´€ë¦¬í•˜ëŠ” ìµœìƒìœ„ ì˜ì‚¬ê²°ì •ì.
    "ì„±ê³µí•˜ë©´ ë” ê¹Šì´, ì‹¤íŒ¨í•˜ë©´ ëŒ€ì•ˆì„ â€” ëì—†ì´ ì—°êµ¬í•˜ëŠ” AI ì—°êµ¬ì†Œ"
    """

    def __init__(
        self,
        agenda_path="experiments/evolution/research_agenda.json",
        history_path="experiments/evolution/history.json",
        tree_search=None,
    ):
        self.agenda_path = agenda_path
        self.history_path = history_path
        self.tree_search = tree_search  # v2.0: AgenticTreeSearch ì¸ìŠ¤í„´ìŠ¤
        self.agenda = self._load_agenda()

        # LLM ì´ˆê¸°í™” (Theoristì™€ ë™ì¼í•œ ë°©ì‹)
        self.model = None
        env_path = os.path.join(
            os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))),
            ".env",
        )
        if os.path.exists(env_path):
            with open(env_path, "r") as f:
                for line in f:
                    if line.startswith("GEMINI_API_KEY="):
                        os.environ["GEMINI_API_KEY"] = line.strip().split("=", 1)[1]
                        break

        api_key = os.getenv("GEMINI_API_KEY")
        if api_key and genai:
            genai.configure(api_key=api_key)
            self.model = genai.GenerativeModel("gemini-2.0-flash")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # í•µì‹¬ ë©”ì„œë“œ
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def get_active_question(self):
        """í˜„ì¬ í™œì„±í™”ëœ ì—°êµ¬ ê³¼ì œë¥¼ ë°˜í™˜. ì—†ìœ¼ë©´ None."""
        questions = self.agenda.get("questions", {})

        # 1. ì´ë¯¸ activeì¸ ê³¼ì œê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ë°˜í™˜
        for q in questions.values():
            if q["status"] == "active":
                return q

        # 2. ì—†ìœ¼ë©´, queued ì¤‘ ì˜ì¡´ì„± ì¶©ì¡± + ìµœê³  ìš°ì„ ìˆœìœ„ë¥¼ í™œì„±í™”
        candidates = []
        for q in questions.values():
            if q["status"] != "queued":
                continue
            # ì˜ì¡´ì„± ê²€ì‚¬
            dep = q.get("depends_on")
            if dep:
                dep_q = questions.get(dep)
                if not dep_q or dep_q["status"] != "completed":
                    continue  # ì˜ì¡´ ê³¼ì œ ë¯¸ì™„ë£Œ â†’ ê±´ë„ˆëœ€
            candidates.append(q)

        if not candidates:
            return None

        # ìš°ì„ ìˆœìœ„ ê¸°ì¤€ ì •ë ¬ (ë‚®ì€ ìˆ«ì = ë†’ì€ ìš°ì„ ìˆœìœ„)
        candidates.sort(key=lambda x: x.get("priority", 99))
        chosen = candidates[0]
        chosen["status"] = "active"
        self._save_agenda()
        print(f"ğŸ“‹ [Director] ìƒˆ ì—°êµ¬ ê³¼ì œ í™œì„±í™”: {chosen['id']} â€” {chosen['question']}")
        return chosen

    def on_generation_complete(self, question_id, results, config):
        """
        í•œ ì„¸ëŒ€ ì™„ë£Œ í›„ í˜¸ì¶œ.
        ë°˜í™˜ê°’:
        - "continue": ê°™ì€ ê³¼ì œ ê³„ì†
        - "success": ëª©í‘œ ë‹¬ì„±
        - "pivot": ì‹¤íŒ¨ íšŸìˆ˜ ì´ˆê³¼, ëŒ€ì•ˆìœ¼ë¡œ ì „í™˜
        """
        question = self.agenda["questions"].get(question_id)
        if not question:
            return "continue"

        # ì„¸ëŒ€ ì¹´ìš´íŠ¸ ì¦ê°€
        question["generation_count"] = question.get("generation_count", 0) + 1
        self.agenda["total_generations_run"] = self.agenda.get("total_generations_run", 0) + 1

        # ìµœê³  ê²°ê³¼ ê°±ì‹ 
        coop = results.get("Prosocial", {}).get("cooperation_rate", 0.0)
        current_best = question.get("best_result") or 0.0
        if coop > current_best:
            question["best_result"] = coop
            question["best_config"] = config

        # ì„±ê³µ íŒì •
        criteria = question.get("success_criteria", {})
        target = criteria.get("target", 0.5)
        condition = criteria.get("condition", ">")

        is_success = False
        if condition == ">":
            is_success = coop > target
        elif condition == ">=":
            is_success = coop >= target

        if is_success:
            self._save_agenda()
            return "success"

        # ì‹¤íŒ¨: ìµœëŒ€ ì„¸ëŒ€ ì´ˆê³¼ í™•ì¸
        max_gen = question.get("constraints", {}).get("max_generations", 50)
        if question["generation_count"] >= max_gen:
            self._save_agenda()
            return "pivot"

        self._save_agenda()
        return "continue"

    def on_success(self, question_id, best_config):
        """
        ì—°êµ¬ ê³¼ì œ ì„±ê³µ ì²˜ë¦¬:
        1. í˜„ì¬ ê³¼ì œë¥¼ 'completed'ë¡œ ë§ˆí‚¹
        2. ì •ì  í›„ì† ê³¼ì œ í™œì„±í™”
        3. LLMìœ¼ë¡œ ë™ì  í›„ì† ê³¼ì œ ìƒì„±
        """
        question = self.agenda["questions"][question_id]
        question["status"] = "completed"
        question["outcome"] = "success"
        question["completed_at"] = datetime.now().isoformat()
        self.agenda["total_questions_completed"] = (
            self.agenda.get("total_questions_completed", 0) + 1
        )

        print(f"\nğŸ‰ [Director] ì—°êµ¬ ì„±ê³µ! {question_id}: {question['question']}")
        print(f"   ìµœê³  í˜‘ë ¥ë¥ : {question.get('best_result', 0):.4f}")
        print(f"   ì†Œìš” ì„¸ëŒ€: {question.get('generation_count', 0)}")

        # ì´ë²¤íŠ¸ ê¸°ë¡
        self._log_event("question_completed", question_id, "success", best_config)

        # ì •ì  í›„ì† ê³¼ì œ í™œì„±í™”
        on_success = question.get("on_success", {})
        if on_success.get("action") == "spawn":
            for next_id in on_success.get("questions", []):
                if next_id in self.agenda["questions"]:
                    next_q = self.agenda["questions"][next_id]
                    if next_q["status"] == "queued":
                        print(f"   â†’ í›„ì† ê³¼ì œ ëŒ€ê¸°: {next_id}")

        # LLMìœ¼ë¡œ ë™ì  í›„ì† ê³¼ì œ ìƒì„±
        if self.model:
            try:
                new_questions = self._generate_followup(question)
                for nq in new_questions:
                    self.agenda["questions"][nq["id"]] = nq
                    print(f"   â†’ ğŸ¤– AI ìƒì„± í›„ì† ê³¼ì œ: {nq['id']} â€” {nq['question']}")
            except Exception as e:
                print(f"   âš ï¸ í›„ì† ê³¼ì œ ìë™ ìƒì„± ì‹¤íŒ¨: {e}")

        # v2.0: íŠ¸ë¦¬ íƒìƒ‰ ëª¨ë“œ â€” ì„±ê³µí•œ ì„¤ì •ì„ ë£¨íŠ¸ë¡œ ê¹Šì´ íƒìƒ‰
        if self.tree_search:
            try:
                root = self.tree_search.create_root(
                    best_config, hypothesis=f"Success on {question_id}"
                )
                children = self.tree_search.expand(root, num_children=2)
                for child in children:
                    qid = self._next_question_id()
                    nq_data = {
                        "question": child.hypothesis,
                        "question_kr": child.hypothesis_kr,
                        "type": "tree_exploration",
                    }
                    full_q = self._make_question(qid, nq_data, parent=question_id)
                    full_q["tree_node_id"] = child.node_id
                    self.agenda["questions"][qid] = full_q
                    print(f"   â†’ ğŸŒ³ íŠ¸ë¦¬ íƒìƒ‰ ê³¼ì œ: {qid} â€” {child.hypothesis_kr}")
                self.tree_search.save_tree()
            except Exception as e:
                print(f"   âš ï¸ íŠ¸ë¦¬ íƒìƒ‰ í™•ì¥ ì‹¤íŒ¨: {e}")

        self._save_agenda()

    def on_failure(self, question_id):
        """
        ì—°êµ¬ ê³¼ì œ ì‹¤íŒ¨ ì²˜ë¦¬:
        1. retry_count ì¦ê°€
        2. max_retries ì´ˆê³¼ ì‹œ fallbackìœ¼ë¡œ ì „í™˜
        3. fallback ì—†ìœ¼ë©´ LLMì—ê²Œ ëŒ€ì•ˆ ìƒì„± ìš”ì²­
        """
        question = self.agenda["questions"][question_id]
        question["retry_count"] = question.get("retry_count", 0) + 1

        on_failure = question.get("on_failure", {})
        max_retries = on_failure.get("max_retries", 2)

        print(f"\nâŒ [Director] ì—°êµ¬ ì‹¤íŒ¨: {question_id} (ì‹œë„ {question['retry_count']}/{max_retries})")
        print(f"   ìµœê³  ê²°ê³¼: {question.get('best_result', 0):.4f}")

        if question["retry_count"] < max_retries:
            # ì¬ì‹œë„: íˆìŠ¤í† ë¦¬ ë¦¬ì…‹ í›„ ê°™ì€ ê³¼ì œ ê³„ì†
            question["generation_count"] = 0
            print(f"   â†’ ì¬ì‹œë„í•©ë‹ˆë‹¤ (íˆìŠ¤í† ë¦¬ ë¦¬ì…‹).")
            if os.path.exists(self.history_path):
                os.remove(self.history_path)
            self._save_agenda()
            return

        # ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼ â†’ ê³¼ì œ ì¢…ë£Œ
        question["status"] = "failed"
        question["outcome"] = "failure"
        question["completed_at"] = datetime.now().isoformat()
        self._log_event("question_failed", question_id, "failure", None)

        # Fallback ê³¼ì œë¡œ ì „í™˜
        action = on_failure.get("action", "archive")
        fallback_id = on_failure.get("fallback_to")

        if action == "fallback" and fallback_id:
            if fallback_id in self.agenda["questions"]:
                fb = self.agenda["questions"][fallback_id]
                if fb["status"] == "queued":
                    fb["status"] = "active"
                    print(f"   â†’ ëŒ€ì•ˆ ê³¼ì œë¡œ ì „í™˜: {fallback_id} â€” {fb['question']}")
        elif action == "generate_new" and self.model:
            # LLMì—ê²Œ ëŒ€ì•ˆ ìƒì„± ìš”ì²­
            try:
                alt = self._generate_alternative(question)
                self.agenda["questions"][alt["id"]] = alt
                alt["status"] = "active"
                print(f"   â†’ ğŸ¤– AI ìƒì„± ëŒ€ì•ˆ ê³¼ì œ: {alt['id']} â€” {alt['question']}")
            except Exception as e:
                print(f"   âš ï¸ ëŒ€ì•ˆ ìë™ ìƒì„± ì‹¤íŒ¨: {e}")
        else:
            print(f"   â†’ ê³¼ì œ ì¢…ë£Œ (ì•„ì¹´ì´ë¸Œë¨).")

        self._save_agenda()

    def generate_new_questions(self):
        """ëª¨ë“  ê³¼ì œê°€ ì†Œì§„ë˜ì—ˆì„ ë•Œ, LLMì—ê²Œ ìƒˆ ì—°êµ¬ ì§ˆë¬¸ ìƒì„±ì„ ìš”ì²­."""
        if not self.model:
            print("âš ï¸ [Director] LLM ì—†ì´ëŠ” ìƒˆ ì§ˆë¬¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ì§€ê¸ˆê¹Œì§€ì˜ ì—°êµ¬ ì„±ê³¼ ìš”ì•½
        completed = [
            q for q in self.agenda["questions"].values() if q["status"] == "completed"
        ]
        failed = [
            q for q in self.agenda["questions"].values() if q["status"] == "failed"
        ]

        summary = {
            "completed": [
                {"question": q["question"], "result": q.get("best_result")} for q in completed
            ],
            "failed": [
                {"question": q["question"], "result": q.get("best_result")} for q in failed
            ],
        }

        prompt = f"""
You are the Research Director of EthicaAI Genesis Lab.

ALL research questions have been exhausted. Here is the research history:
{json.dumps(summary, indent=2)}

Based on what we've learned, propose 2 entirely NEW research directions.
Consider:
1. What patterns emerged from successes and failures?
2. What fundamental assumptions haven't been tested?
3. What would be a breakthrough discovery?

Output JSON array:
[
  {{
    "question": "A concise research question in English",
    "question_kr": "í•œêµ­ì–´ ë²„ì „",
    "type": "exploration",
    "success_criteria": {{"metric": "cooperation_rate", "condition": ">", "target": 0.5}},
    "rationale": "Why this question matters",
    "rationale_kr": "ì™œ ì´ ì§ˆë¬¸ì´ ì¤‘ìš”í•œì§€ í•œêµ­ì–´ ì„¤ëª…"
  }}
]
"""
        try:
            response = self.model.generate_content(prompt)
            text = response.text.replace("```json", "").replace("```", "").strip()
            new_qs = json.loads(text)

            for i, nq in enumerate(new_qs):
                qid = self._next_question_id()
                full_q = self._make_question(qid, nq)
                self.agenda["questions"][qid] = full_q
                print(f"ğŸ¤– [Director] ìƒˆ ì—°êµ¬ ë°©í–¥ ìƒì„±: {qid} â€” {full_q['question']}")

            self._save_agenda()
        except Exception as e:
            print(f"âš ï¸ [Director] ìƒˆ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")

    def get_progress_summary(self):
        """ëŒ€ì‹œë³´ë“œìš© ì—°êµ¬ ì§„í–‰ ìƒí™© ìš”ì•½."""
        questions = self.agenda.get("questions", {})
        total = len(questions)
        completed = sum(1 for q in questions.values() if q["status"] == "completed")
        failed = sum(1 for q in questions.values() if q["status"] == "failed")
        active = [q for q in questions.values() if q["status"] == "active"]

        return {
            "total": total,
            "completed": completed,
            "failed": failed,
            "active": active[0] if active else None,
            "total_generations": self.agenda.get("total_generations_run", 0),
            "progress_pct": (completed / total * 100) if total > 0 else 0,
        }

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # LLM ê¸°ë°˜ ê³¼ì œ ìë™ ìƒì„±
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _generate_followup(self, completed_question):
        """ì„±ê³µí•œ ê³¼ì œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í›„ì† ì—°êµ¬ ê³¼ì œ ìƒì„±."""
        prompt = f"""
You are the Research Director of EthicaAI Genesis Lab.

A research question has been SUCCESSFULLY answered:
- Question: {completed_question['question']}
- Best Cooperation Rate: {completed_question.get('best_result', 0):.4f}
- Generations Used: {completed_question.get('generation_count', 0)}

Based on this success, propose 2 follow-up research questions.
Consider:
1. Can we push the result higher? (Goal escalation)
2. Does this generalize? (Robustness test)
3. WHY did it work? (Mechanistic understanding)

Output JSON array:
[
  {{
    "question": "A concise research question in English",
    "question_kr": "í•œêµ­ì–´ ë²„ì „",
    "type": "escalation|generalization|analysis",
    "success_criteria": {{"metric": "cooperation_rate", "condition": ">", "target": float}},
    "rationale": "Why this question matters",
    "rationale_kr": "ì™œ ì´ ì§ˆë¬¸ì´ ì¤‘ìš”í•œì§€ í•œêµ­ì–´ ì„¤ëª…"
  }}
]
"""
        response = self.model.generate_content(prompt)
        text = response.text.replace("```json", "").replace("```", "").strip()
        raw_questions = json.loads(text)

        result = []
        for nq in raw_questions:
            qid = self._next_question_id()
            full_q = self._make_question(qid, nq, parent=completed_question["id"])
            result.append(full_q)
        return result

    def _generate_alternative(self, failed_question):
        """ì‹¤íŒ¨í•œ ê³¼ì œì— ëŒ€í•œ ëŒ€ì•ˆ ì—°êµ¬ ê³¼ì œ ìƒì„±."""
        prompt = f"""
You are the Research Director of EthicaAI Genesis Lab.

A research question has FAILED after {failed_question.get('generation_count', 0)} generations:
- Question: {failed_question['question']}
- Best result: {failed_question.get('best_result', 0):.4f}
- Target was: {failed_question.get('success_criteria', {}).get('target', 0.5)}

The current approach isn't working. Propose 1 ALTERNATIVE approach.
Consider:
1. Changing the environment or reward structure
2. A completely different optimization strategy
3. Relaxing constraints or reframing the problem

Output JSON:
{{
  "question": "A concise alternative research question",
  "question_kr": "í•œêµ­ì–´ ë²„ì „",
  "type": "pivot",
  "success_criteria": {{"metric": "cooperation_rate", "condition": ">", "target": float}},
  "rationale": "Why this alternative might work",
  "rationale_kr": "ì™œ ì´ ëŒ€ì•ˆì´ íš¨ê³¼ì ì¼ ìˆ˜ ìˆëŠ”ì§€ í•œêµ­ì–´ ì„¤ëª…"
}}
"""
        response = self.model.generate_content(prompt)
        text = response.text.replace("```json", "").replace("```", "").strip()
        raw = json.loads(text)

        qid = self._next_question_id()
        return self._make_question(qid, raw, parent=failed_question["id"])

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ë³´ì¡° ë©”ì„œë“œ
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _load_agenda(self):
        """ì—°êµ¬ ì˜ì œë¥¼ JSONì—ì„œ ë¡œë“œ."""
        if os.path.exists(self.agenda_path):
            with open(self.agenda_path, "r", encoding="utf-8") as f:
                return json.load(f)
        # íŒŒì¼ ì—†ìœ¼ë©´ ë¹ˆ ì˜ì œ ìƒì„±
        return {
            "lab_name": "EthicaAI Genesis Lab",
            "version": "1.0",
            "created_at": datetime.now().isoformat(),
            "total_generations_run": 0,
            "total_questions_completed": 0,
            "questions": {},
            "history": [],
        }

    def _save_agenda(self):
        """ì—°êµ¬ ì˜ì œë¥¼ JSONìœ¼ë¡œ ì €ì¥."""
        os.makedirs(os.path.dirname(self.agenda_path), exist_ok=True)
        with open(self.agenda_path, "w", encoding="utf-8") as f:
            json.dump(self.agenda, f, indent=2, ensure_ascii=False)

    def _log_event(self, event_type, question_id, outcome, config):
        """ì—°êµ¬ ì´ë²¤íŠ¸ë¥¼ íˆìŠ¤í† ë¦¬ì— ê¸°ë¡."""
        event = {
            "timestamp": datetime.now().isoformat(),
            "event": event_type,
            "question_id": question_id,
            "outcome": outcome,
        }
        if config:
            event["best_config"] = {
                k: v for k, v in config.items() if k.startswith("GENESIS_")
            }
        self.agenda.setdefault("history", []).append(event)

    def _next_question_id(self):
        """ë‹¤ìŒ ì—°êµ¬ ê³¼ì œ ID ìƒì„± (RQ-XXX)."""
        existing_ids = list(self.agenda.get("questions", {}).keys())
        if not existing_ids:
            return "RQ-001"
        max_num = max(int(qid.split("-")[1]) for qid in existing_ids if qid.startswith("RQ-"))
        return f"RQ-{max_num + 1:03d}"

    def _make_question(self, qid, raw, parent=None):
        """LLM ì¶œë ¥ì„ ì •ê·œ ì—°êµ¬ ê³¼ì œ êµ¬ì¡°ë¡œ ë³€í™˜."""
        criteria = raw.get("success_criteria", {"metric": "cooperation_rate", "condition": ">", "target": 0.5})

        return {
            "id": qid,
            "question": raw.get("question_kr", raw.get("question", "ìë™ ìƒì„±ëœ ì—°êµ¬ ê³¼ì œ")),
            "question_en": raw.get("question", "Auto-generated research question"),
            "type": raw.get("type", "exploration"),
            "status": "queued",
            "priority": 2,
            "created_at": datetime.now().isoformat(),
            "success_criteria": criteria,
            "constraints": {
                "max_generations": 40,
                "parameter_space": {
                    "GENESIS_BETA": [0.01, 100.0],
                    "GENESIS_ALPHA": [0.01, 5.0],
                    "GENESIS_LOGIC_MODE": ["adaptive_beta", "inverse_beta", "institutional"],
                    "IA_ALPHA": [0.1, 10.0],
                    "IA_BETA": [0.01, 1.0],
                    "USE_INEQUITY_AVERSION": [True, False],
                },
            },
            "on_success": {"action": "spawn", "questions": []},
            "on_failure": {
                "action": "generate_new",
                "fallback_to": None,
                "max_retries": 1,
            },
            "depends_on": None,
            "parent": parent,
            "generation_count": 0,
            "retry_count": 0,
            "best_result": None,
            "completed_at": None,
            "outcome": None,
        }


if __name__ == "__main__":
    director = ResearchDirector()
    summary = director.get_progress_summary()
    print(f"ğŸ“Š ì—°êµ¬ì†Œ í˜„í™©: {json.dumps(summary, indent=2, ensure_ascii=False, default=str)}")

    q = director.get_active_question()
    if q:
        print(f"ğŸ“‹ í˜„ì¬ í™œì„± ê³¼ì œ: {q['id']} â€” {q['question']}")
    else:
        print("ğŸ“‹ í™œì„± ê³¼ì œ ì—†ìŒ. ìƒˆ ì§ˆë¬¸ ìƒì„± ì¤‘...")
        director.generate_new_questions()
