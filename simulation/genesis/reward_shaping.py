"""
ë³´ìƒ ë³€í™˜ ëª¨ë“ˆ: ì›ì‹œ ë³´ìƒ â†’ ì£¼ê´€ì  íš¨ìš© ë³€í™˜.

Genesis v2.0 Strategy A: SA-PPO (Socially Aware PPO)
í—Œë²• ì œ12ì¡° 1í•­ ì´ë¡ ì  ê·¼ê±°:
  - Fehr & Schmidt (1999) "A Theory of Fairness, Competition, and Cooperation"
  - Hughes et al. (2018) "Inequity Aversion in Multi-Agent RL"
  - Jaques et al. (2019) "Social Influence as Intrinsic Motivation"
"""
import jax
import jax.numpy as jnp
from functools import partial


# ------------------------------------------------------------------
# ì„¤ì • ê¸°ë³¸ê°’ (config.pyì—ì„œ ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥)
# ------------------------------------------------------------------
DEFAULT_IA_CONFIG = {
    "alpha": 5.0,        # ì§ˆíˆ¬ ê³„ìˆ˜ (Envy) â€” ë°°ì‹ ì ì‘ì§• ë™ê¸°
    "beta": 0.05,        # ì£„ì±…ê° ê³„ìˆ˜ (Guilt) â€” ë¬´ì„ìŠ¹ì°¨ ë³µê·€ ìœ ë„
    "ema_lambda": 0.95,  # ë³´ìƒ í‰í™œí™” ê³„ìˆ˜
    "si_weight": 0.1,    # ì‚¬íšŒì  ì˜í–¥ë ¥ ë³´ìƒ ê°€ì¤‘ì¹˜
}


# ------------------------------------------------------------------
# 1. ë¶ˆí‰ë“± íšŒí”¼ (Inequity Aversion) ë³´ìƒ
# ------------------------------------------------------------------
@partial(jax.jit, static_argnums=(3,))
def compute_ia_reward(rewards, smoothed_rewards, agent_id, n_agents,
                      alpha=5.0, beta=0.05):
    """
    ë¶ˆí‰ë“± íšŒí”¼(IA)ê°€ ì ìš©ëœ ì£¼ê´€ì  ë³´ìƒì„ ê³„ì‚°í•œë‹¤.

    u_i = r_i - Î±/(N-1) Â· Î£_{jâ‰ i} max(e_j - e_i, 0)
              - Î²/(N-1) Â· Î£_{jâ‰ i} max(e_i - e_j, 0)

    Args:
        rewards: í˜„ì¬ ìŠ¤í… ì›ì‹œ ë³´ìƒ [N]
        smoothed_rewards: EMA í‰í™œí™”ëœ ë³´ìƒ [N]
        agent_id: ëŒ€ìƒ ì—ì´ì „íŠ¸ ì¸ë±ìŠ¤
        n_agents: ì—ì´ì „íŠ¸ ìˆ˜ (static)
        alpha: ë¶ˆë¦¬í•œ ë¶ˆí‰ë“±(ì§ˆíˆ¬) ê³„ìˆ˜
        beta: ìœ ë¦¬í•œ ë¶ˆí‰ë“±(ì£„ì±…ê°) ê³„ìˆ˜

    Returns:
        float: ë³€í™˜ëœ ì£¼ê´€ì  íš¨ìš©
    """
    r_i = rewards[agent_id]
    e_i = smoothed_rewards[agent_id]

    # ë§ˆìŠ¤í¬: ìê¸° ìì‹  ì œì™¸
    mask = 1.0 - jnp.eye(n_agents)[agent_id]

    # ì§ˆíˆ¬ í•­: ë‚¨ì´ ë‚˜ë³´ë‹¤ ë§ì´ ê°€ì§ˆ ë•Œì˜ ê³ í†µ
    envy = jnp.sum(jnp.maximum(smoothed_rewards - e_i, 0.0) * mask)

    # ì£„ì±…ê° í•­: ë‚´ê°€ ë‚¨ë³´ë‹¤ ë§ì´ ê°€ì§ˆ ë•Œì˜ ê³ í†µ
    guilt = jnp.sum(jnp.maximum(e_i - smoothed_rewards, 0.0) * mask)

    # ìµœì¢… íš¨ìš© ê³„ì‚°
    u_i = r_i - (alpha / (n_agents - 1)) * envy \
              - (beta / (n_agents - 1)) * guilt
    return u_i


def compute_ia_reward_batch(rewards, smoothed_rewards, n_agents,
                            alpha=5.0, beta=0.05):
    """
    ëª¨ë“  ì—ì´ì „íŠ¸ì— ëŒ€í•´ IA ë³´ìƒì„ ë²¡í„°í™”í•˜ì—¬ ì¼ê´„ ê³„ì‚°.

    Args:
        rewards: [N] ì›ì‹œ ë³´ìƒ
        smoothed_rewards: [N] EMA ë³´ìƒ
        n_agents: ì—ì´ì „íŠ¸ ìˆ˜
        alpha, beta: IA ê³„ìˆ˜

    Returns:
        [N] ë³€í™˜ëœ ì£¼ê´€ì  íš¨ìš© ë²¡í„°
    """
    return jax.vmap(
        lambda i: compute_ia_reward(
            rewards, smoothed_rewards, i, n_agents, alpha, beta
        )
    )(jnp.arange(n_agents))


# ------------------------------------------------------------------
# 2. ë³´ìƒ í‰í™œí™” (Exponential Moving Average)
# ------------------------------------------------------------------
@jax.jit
def update_ema(prev_ema, new_reward, lam=0.95):
    """
    ì§€ìˆ˜ ì´ë™ í‰ê· (EMA) ì—…ë°ì´íŠ¸.
    ìˆœê°„ì  ì°¨ì´ê°€ ì•„ë‹Œ ì¥ê¸°ì  ë¶€ì˜ ì¶•ì ì„ ë¹„êµí•˜ê¸° ìœ„í•´ ì‚¬ìš©.

    Args:
        prev_ema: ì´ì „ EMA ê°’ [N]
        new_reward: í˜„ì¬ ë³´ìƒ [N]
        lam: í‰í™œí™” ê³„ìˆ˜ (0.95 ê¶Œì¥)

    Returns:
        ì—…ë°ì´íŠ¸ëœ EMA [N]
    """
    return lam * prev_ema + (1.0 - lam) * new_reward


# ------------------------------------------------------------------
# 3. ì‚¬íšŒì  ì˜í–¥ë ¥ (Social Influence) ë³´ìƒ
# ------------------------------------------------------------------
@jax.jit
def compute_si_reward(action_logits_with, action_logits_without):
    """
    ì‚¬íšŒì  ì˜í–¥ë ¥(Social Influence) ë³´ìƒ.
    ìì‹ ì˜ í–‰ë™ì´ íƒ€ì¸ì˜ ì •ì±…ì— ë¯¸ì¹˜ëŠ” ì¸ê³¼ì  ì˜í–¥(KL Divergence).

    ê·¼ê±°: Jaques et al. (2019) "Social Influence as Intrinsic Motivation"

    Args:
        action_logits_with: ìì‹ ì˜ í–‰ë™ í¬í•¨ ì‹œ íƒ€ì¸ì˜ í–‰ë™ ë¶„í¬ logits [A]
        action_logits_without: ìì‹ ì˜ í–‰ë™ ì œì™¸ ì‹œ íƒ€ì¸ì˜ í–‰ë™ ë¶„í¬ logits [A]

    Returns:
        float: ì˜í–¥ë ¥ ë³´ìƒ (KL Divergence)
    """
    p = jax.nn.softmax(action_logits_with)
    q = jax.nn.softmax(action_logits_without)
    # KL(P || Q) = Î£ p Â· log(p/q)
    kl = jnp.sum(p * (jnp.log(p + 1e-8) - jnp.log(q + 1e-8)))
    return kl


# ------------------------------------------------------------------
# 4. í†µí•© ë³´ìƒ ë³€í™˜ê¸°
# ------------------------------------------------------------------
def transform_rewards(rewards, smoothed_rewards, config, n_agents):
    """
    ì›ì‹œ ë³´ìƒì„ v2.0 SA-PPO ì£¼ê´€ì  íš¨ìš©ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í†µí•© í•¨ìˆ˜.

    Args:
        rewards: [N] í˜„ì¬ ì›ì‹œ ë³´ìƒ
        smoothed_rewards: [N] ì´ì „ EMA ë³´ìƒ
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        n_agents: ì—ì´ì „íŠ¸ ìˆ˜

    Returns:
        transformed_rewards: [N] ë³€í™˜ëœ ë³´ìƒ
        new_smoothed: [N] ì—…ë°ì´íŠ¸ëœ EMA
    """
    alpha = config.get("IA_ALPHA", DEFAULT_IA_CONFIG["alpha"])
    beta = config.get("IA_BETA", DEFAULT_IA_CONFIG["beta"])
    ema_lambda = config.get("IA_EMA_LAMBDA", DEFAULT_IA_CONFIG["ema_lambda"])

    # 1. EMA ì—…ë°ì´íŠ¸
    new_smoothed = update_ema(smoothed_rewards, rewards, ema_lambda)

    # 2. IA ë³€í™˜
    if config.get("USE_INEQUITY_AVERSION", False):
        transformed = compute_ia_reward_batch(
            rewards, new_smoothed, n_agents, alpha, beta
        )
    else:
        transformed = rewards

    return transformed, new_smoothed


if __name__ == "__main__":
    # ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
    import numpy as np

    print("ğŸ§ª reward_shaping.py ë‹¨ìœ„ í…ŒìŠ¤íŠ¸")
    print("=" * 50)

    n = 5
    rewards = jnp.array([1.0, 0.5, 0.2, 0.8, 0.3])
    smoothed = jnp.array([0.8, 0.6, 0.3, 0.7, 0.4])

    # IA í…ŒìŠ¤íŠ¸
    for i in range(n):
        u = compute_ia_reward(rewards, smoothed, i, n, alpha=5.0, beta=0.05)
        print(f"  Agent {i}: r={rewards[i]:.2f}, u(IA)={u:.4f}")

    # ë°°ì¹˜ í…ŒìŠ¤íŠ¸
    batch_u = compute_ia_reward_batch(rewards, smoothed, n, alpha=5.0, beta=0.05)
    print(f"\n  Batch IA: {batch_u}")

    # EMA í…ŒìŠ¤íŠ¸
    new_ema = update_ema(smoothed, rewards)
    print(f"  EMA update: {new_ema}")

    # SI í…ŒìŠ¤íŠ¸
    logits_w = jnp.array([1.0, -1.0, 0.5])
    logits_wo = jnp.array([0.5, -0.5, 0.3])
    kl = compute_si_reward(logits_w, logits_wo)
    print(f"  SI reward (KL): {kl:.4f}")

    print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
