# 다중 에이전트 강화학습을 통한 아마르티아 센의 최적 합리성 이론 검증

**허예솔**  
독립 연구자, 서울, 대한민국  
연락처: dpfh1537@gmail.com

## 초록

인공지능 에이전트를 인간 사회에 통합하기 위해서는 자기 이익과 사회적 가치 사이의 근본적 갈등을 해결해야 합니다. 본 연구는 아마르티아 센의 **"메타-랭킹(Meta-Ranking)"** 이론—선호에 대한 선호—을 다중 에이전트 강화학습(MARL) 프레임워크 내에서 공식화합니다. 7가지 사회적 가치 지향성(SVO)을 가진 에이전트를 3가지 환경(Cleanup, 반복 죄수의 딜레마, 공공재 게임)에서 최대 100 에이전트 규모로 시뮬레이션했습니다.

**5가지 핵심 발견:**

1. **동적 메타-랭킹(λ_t)이 집단 복지를 유의하게 향상** (p=0.0003)하는 반면, 정적 SVO 주입은 실패합니다.
2. 에이전트가 **자발적 역할 전문화**를 보이며 "청소자"와 "섭취자"로 분화합니다.
3. **"상황적 헌신(Situational Commitment)"만이 진화적 안정 전략(ESS)으로 생존**하며, 초기 조건과 무관하게 집단의 ~12%로 수렴합니다.
4. PGG에서 **개인주의적 SVO(θ=15°)의 에이전트가 인간 행동 데이터와 가장 유사**합니다 (WD=0.053). 이는 센의 "합리적 바보" 비판을 검증합니다.
5. **완전 요인 분해 결과, SVO 회전이 효과의 86%**, 동적 λ가 13%를 차지합니다.

**핵심어**: 최적 합리성, 메타-랭킹, 다중 에이전트 강화학습, 사회적 가치 지향성, 아마르티아 센, 진화적 안정 전략

---

## 1. 서론

### 1.1 배경: "합리적 바보"를 넘어서

합리적 선택 이론(RCT)은 고전 경제학과 게임 이론의 근간으로, 인간 행동을 효용 극대화 과정으로 정의합니다. 그러나 센(1977)은 *합리적 바보(Rational Fools)*에서 이 정의가 인간 행위의 복잡성을 과도하게 단순화한다고 비판했습니다. "동정심(Sympathy)"—타인의 복지를 자신의 효용 함수에 통합하는 것—과 "헌신(Commitment)"—개인 복지를 희생하면서 도덕적 원칙을 고수하는 것—은 근본적으로 다른 메커니즘입니다.

RCT는 이 둘을 단일 선호 순서로 환원함으로써 동어반복적 오류를 범합니다. 이를 극복하기 위해 센은 **메타-랭킹**—선호 자체에 대한 선호를 순위 매기는 구조—을 제안했습니다.

### 1.2 연구 목적

본 연구는 센의 철학적 통찰을 **계산 사회과학(CSS)** 및 **다중 에이전트 강화학습(MARL)** 모델로 번역하여, 최적 합리성이 사회적 딜레마에서 어떻게 발현되는지 검토합니다.

1. **수학적 공식화**: 메타-랭킹 이론을 강화학습의 보상 함수 구조로 번역
2. **대규모 시뮬레이션**: JAX 기반 GPU 가속 환경에서 최대 100 에이전트 사회적 딜레마 실험
3. **인과적 추론 검증**: OLS, HAC, LMM을 통한 엄밀한 통계적 검증
4. **교차 환경 검증**: Cleanup, IPD, PGG 3가지 환경에서의 일반화 확인

### 1.3 기여

| 기여 | 내용 |
|------|------|
| C1 | 센의 메타-랭킹을 MARL에 최초 적용한 프레임워크 |
| C2 | 역할 전문화의 자발적 출현 발견 |
| C3 | 조건부 헌신의 진화적 안정성 증명 |
| C4 | 20 → 100 에이전트 확장 시 효과 크기 초선형 증가 확인 |
| C5 | 인간 PGG 데이터와의 정량적 유사성 검증 (WD < 0.2) |
| C6 | Cleanup, IPD, PGG 3개 환경 교차 검증 |
| C7 | 복제자 동역학을 통한 ~12% ESS 수렴 증명 |

---

## 2. 방법

### 2.1 메타-랭킹 보상 함수

센의 이론을 강화학습 프레임워크 내에서 공식화합니다. 에이전트 $i$의 총 보상:

$$R_{total}^{(i)} = (1 - \lambda_t^{(i)}) \cdot U_{self}^{(i)} + \lambda_t^{(i)} \cdot [U_{meta}^{(i)} - \psi^{(i)}]$$

여기서:
- $U_{self}^{(i)}$: 개인 보상 (환경 + 항상성 드라이브)
- $U_{meta}^{(i)} = \frac{1}{N-1}\sum_{j \neq i} U_{self}^{(j)}$: 다른 에이전트의 평균 보상 (동정심 항)
- $\psi^{(i)} = \beta \cdot |U_{self}^{(i)} - U_{meta}^{(i)}|$: 자제 비용
- $\lambda_t^{(i)}$: 동적 헌신 계수

### 2.2 동적 λ 메커니즘

$$\lambda_t = \begin{cases} 0 & \text{생존 위기 시 } (w < w_{survival}) \\ \min(1, 1.5 \cdot \lambda_{base}) & \text{여유 모드 } (w > w_{boost}) \\ \lambda_{base} = \sin(\theta) & \text{일반 모드} \end{cases}$$

이 메커니즘은 극단적 결핍 하에서는 헌신이 불가능하고, 풍요 속에서는 촉진된다는 센의 통찰을 수학적으로 구현합니다.

---

## 3. 핵심 결과

### 3.1 동적 메타-랭킹의 효과

| 실험 조건 | H1 p-value | 효과 크기 (f²) | 해석 |
|-----------|-----------|--------------|------|
| Full Model | **0.0023** | 1.75 | ✅ 유의 (HAC Robust) |
| Baseline (OFF) | 0.64 | — | ❌ 비유의 → 메타-랭킹이 핵심 |
| No-Psi (ψ=0) | 0.42 | — | ❌ ψ 기여도 낮음 |
| Static-Lambda | 0.015 | 2.53 | ✅ 부분적 효과 |

### 3.2 확장성 검증 (100 에이전트)

- **통계적 유의성 강화**: 보상에 대한 SVO 효과가 더 유의 ($p=0.0003$)
- **협력 효과 출현**: 20 에이전트에서 비유의($p=0.18$)였던 협력률이 100 에이전트에서 유의($p<0.0001$)
- **초선형 불평등 감소**: Gini Cohen's $f^2$가 5.79 → **10.21**로 증가

### 3.3 강건성 분석

- **수렴 검증**: ADF 검정 p<0.05, **87%(61/70) runs 수렴**
- **위험 조정 비교**: Dynamic λ의 CV=0.109 < Static λ의 CV=0.161
- **민감도 분석**: 5/7 SVO 조건에서 유의

### 3.4 교차 환경 검증

| SVO 조건 | Cleanup | IPD | 효과 |
|---------|---------|-----|------|
| 이기적 (0°) | 0.0 | 0.0 | 없음 |
| 경쟁적 (30°) | n/a | +17.8% | **강함** |
| 친사회적 (45°) | +3.2% | +10.0% | 강함 |
| 완전 이타적 (90°) | 0.0 | 0.0 | 천장 |

### 3.5 공공재 게임: 인간 데이터와의 구조적 정합

N-Player PGG ($N=4$, 승수$=1.6$, 10 라운드)에서 **개인주의적 SVO(θ=15°)**가 인간 행동 데이터와 가장 낮은 괴리를 보임 ($WD=0.053$). 가장 이타적인 에이전트가 아닌, 적당히 이기적인 에이전트가 인간에 가장 가까움 — 이는 센의 통찰(순수 이타주의는 인간의 규범이 아니며, **제한된 헌신**이 핵심)을 검증합니다.

### 3.6 진화적 안정: "도덕적 소수" 가설

복제자 동역학(200세대, $N=100$, 5개 초기 비율 × 10 시드)을 통해, 메타-랭킹 전략이 초기 조건과 무관하게 집단의 **약 12%**로 수렴함을 발견했습니다. 소수의 "도덕적 리더"가 집단 복지를 유지하는 ESS — 인간 협력 실험에서 관찰되는 "임계 질량"(Granovetter, 1978)의 계산적 유사체입니다.

### 3.7 메커니즘 분해

완전 요인 분석($2^3 = 8$ 조건)으로 메타-랭킹을 3개 구성 요소로 분해:

| 구성 요소 | 기여율 | 비율 |
|----------|-------|------|
| SVO 회전 | +0.79 | 86% |
| 동적 λ | +0.12 | 13% |
| 자제 비용 ψ | -0.03 | 1% |

핵심: 동적 λ만으로는 협력을 생성할 수 없으며, 기존 친사회적 성향의 **증폭기(amplifier)** 역할을 합니다.

### 3.8 전체 환경 스윕: 사회적 딜레마 간 일반화

4개 환경 × 7 SVO × 10 시드 = **560회 실험**(Fig. 24)을 통해 메타-랭킹의 광범위한 일반화를 확인했습니다.

| 환경 | 최대 ATE(협력) | 최적 SVO | ATE(보상) |
|:---:|:-:|:-:|:-:|
| Cleanup | +0.083 | 협력적 (60°) | — |
| IPD | 0.000 | — | — |
| **PGG** | **+0.211** | **친사회적 (45°)** | **+2.535** |
| **Harvest** | **+0.506** | **이기적 (0°)** | **+0.101** |

**공유 자원 동역학** 환경(PGG, Harvest)에서 가장 강한 효과. Harvest에서 이기적 에이전트가 최대 ATE를 보인 점이 주목할 만합니다 — 메타-랭킹의 생존 위기 λ 억제가 과다 수확을 정확히 방지했습니다.

### 3.9 이종 SVO 집단: 임계점 가설

**혼합 SVO 집단**(친사회적 비율 0~100%)에서 비선형 **임계점**(~30%)을 발견했습니다 (Fig. 25~26). PGG에서 최대 복지 개선 **ΔW = +10,080** (prosocial 100%). Section 3.6의 ESS ~12%와 일관됩니다.

### 3.10 커뮤니케이션 채널: Cheap Talk과 진실성

1비트 메시지 채널 추가 시, prosocial SVO에서 협력률 **+5.8% 향상** (Fig. 27~28). 메시지 진실성이 **~98%로 수렴** — 메타-랭킹 하에서 정직한 신호가 진화적으로 유리함을 입증했습니다.

### 3.11 연속 행동 공간: 이진 결정을 넘어

PGG를 연속 기여($c_i \in [0, E]$)로 확장 시에도 메타-랭킹 ATE ≈ **+0.20** 유지 (Fig. 29~30). Beta 분포 정책을 통해 이산/연속 환경 모두에서 일관된 효과를 확인했습니다.

---

## 4. 결론: 호모 이코노미쿠스를 넘어서

본 연구의 3가지 함의:

1. **AI 정렬**: 시스템은 정적 가치를 인코딩하는 것이 아닌, *언제* 도덕적이어야 하는지를 학습해야 합니다.
2. **행동 경제학**: 제한된 이기심(θ=15°)의 에이전트가 인간 행동을 가장 잘 복제하며 ($WD=0.053$), 센의 "합리적 바보" 비판을 검증합니다.
3. **진화 이론**: 도덕적 행동이 보편적일 필요 없이, ~12%의 "도덕적 소수"로 ESS가 됩니다.

우리가 제안하는 "최적 합리적 에이전트"는 완벽한 성인이 아닙니다. 생존을 걱정하고 계산적으로 헌신합니다. 그러나 역설적으로, 바로 그 때문에 **지속 가능한 도덕성**을 실현할 수 있습니다.

---

## 참고문헌

1. Sen, A. (1977). Rational Fools: A Critique of the Behavioral Foundations of Economic Theory. *Philosophy & Public Affairs*, 6(4), 317-344.
2. Fehr, E. & Gächter, S. (2000). Cooperation and Punishment in Public Goods Experiments. *American Economic Review*, 90(4), 980-994.
3. Granovetter, M. (1978). Threshold Models of Collective Behavior. *American Journal of Sociology*, 83(6), 1420-1443.
4. Schwarting, W., et al. (2019). Social Value Orientation and Self-Driving Cars. *RSS*.
5. McKee, K. R., et al. (2020). Social Diversity and Social Preferences in Mixed-Motive Reinforcement Learning. *AAMAS*.
6. Keramati, M. & Gutkin, B. (2014). Homeostatic reinforcement learning. *eLife*, 3, e04811.
7. Russell, S. (2019). *Human Compatible*. Viking.
8. Weibull, J. W. (1995). *Evolutionary Game Theory*. MIT Press.
9. Santos, F. C., et al. (2008). Social diversity promotes cooperation. *Nature*, 454(7201), 213-216.
