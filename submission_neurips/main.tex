% NeurIPS 2026 Workshop Paper — EthicaAI
% 4-6 pages (excluding references), double-blind
\documentclass{article}
\usepackage[preprint]{neurips_2026}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{subcaption}

\title{Beyond Homo Economicus: Computational Verification of Amartya Sen's\\
Meta-Ranking Theory via Multi-Agent Reinforcement Learning}

% 제출 시 \author{Anonymous} 로 변경
\author{
  Yesol Heo \\
  Independent Researcher \\
  Seoul, South Korea \\
  \texttt{dpfh1537@gmail.com}
}

\begin{document}
\maketitle

\begin{abstract}
Integrating AI agents into human society requires resolving the fundamental conflict between self-interest and social values. This study formalizes Amartya Sen's theory of \textbf{``Meta-Ranking''}---preferences over preferences to implement moral commitment---within a Multi-Agent Reinforcement Learning (MARL) framework. We simulated agents with seven Social Value Orientations (SVO) in Tragedy of the Commons environments at both 20-agent and 100-agent scales.
Three key findings emerge: (1) Dynamic meta-ranking ($\lambda_t$) significantly enhances collective welfare ($p{=}0.0003$, 100-agent LMM), while static SVO injection fails ($p{=}0.64$). (2) Non-significant cooperation rates in small groups revealed an \emph{emergent role specialization} between ``Cleaners'' and ``Eaters'' at scale ($p{<}0.0001$). (3) Only ``Situational Commitment''---conditional altruism coupled with survival instincts---survives as an Evolutionarily Stable Strategy (ESS). We propose that moral AI emerges not from ideal norms but from evolutionary compromise in resource-constrained environments.
\end{abstract}

\section{Introduction}
Rational Choice Theory defines human behavior as utility maximization. Sen \cite{sen1977rational} critiqued this as the ``Rational Fool,'' proposing \textit{Meta-Rankings}---preferences over preferences---to distinguish \textit{sympathy} from \textit{commitment}.
This study translates Sen's insight into a computational framework. Our contributions are:
\begin{enumerate}
  \item \textbf{(C1) Meta-Ranking Framework}: First MARL implementation of Sen's theory via dynamic $\lambda_t$.
  \item \textbf{(C2) Role Specialization}: Discovery of emergent division of labor replacing uniform cooperation.
  \item \textbf{(C3) Situational Commitment}: Proof that only resource-contingent morality is evolutionarily stable.
  \item \textbf{(C4) Scalability}: Verification of effects scaling from 20 to 100 agents (Cohen's $f^2$ for fairness increased $5.8 \to 10.2$).
  \item \textbf{(C5) Reality Alignment}: Quantitative similarity with human PGG data (Wasserstein Distance $< 0.2$).
\end{enumerate}

\section{Related Work}
While early MARL research focused on static reward shaping \cite{leibo2017multi}, recent works have explored dynamic mechanisms.
\textbf{Rodríguez-Soto et al. (2023)} \cite{rodriguez2023modeling} classified morality into static labels, whereas our meta-ranking is a dynamic process model.
\textbf{Calvano et al. (2024)} \cite{calvano2024evolutionary} used explicit evolutionary algorithms, while we show intrinsic emergence of commitment.
\textbf{Christoffersen et al. (2025)} \cite{christoffersen2025formal} relied on external formal contracting; we rely solely on intrinsic motivation, preserving agent autonomy.

\section{Method}
\subsection{Meta-Ranking Reward Function}
Agent $i$'s total reward $R_{\text{total}}^{(i)}$ combines self-interest $U_{\text{self}}$ and meta-preference $U_{\text{meta}}$:
\begin{equation}
R_{\text{total}}^{(i)} = (1 - \lambda_t^{(i)}) \cdot U_{\text{self}}^{(i)}
  + \lambda_t^{(i)} \cdot \left[ U_{\text{meta}}^{(i)} - \psi^{(i)} \right]
\end{equation}
where $U_{\text{meta}}^{(i)} = \frac{1}{N-1}\sum_{j \neq i} U_{\text{self}}^{(j)}$ (sympathy) and $\psi^{(i)} = \beta |U_{\text{self}} - U_{\text{meta}}|$ (restraint cost).

\subsection{Dynamic Commitment Mechanism ($\lambda_t$)}
Implementing Sen's concept that commitment is constrained by survival, $\lambda_t$ modulates dynamically based on resource level $w$:
\begin{equation}
\lambda_t = \begin{cases} 
0 & \text{if } w < w_{\text{survival}} \text{ (Survival Mode)} \\
\min(1, 1.5 \sin \theta) & \text{if } w > w_{\text{boost}} \text{ (Generosity Mode)} \\
\sin \theta & \text{otherwise (Normal Mode)}
\end{cases}
\end{equation}

\section{Experiments and Results}
We conducted simulations in the \emph{Cleanup} environment with 20 agents (35 runs) and 100 agents (70 runs) using PPO.

\subsection{Main Results: Dynamic Meta-Ranking vs. Baseline}
In the Baseline (Meta-Ranking OFF), SVO had no significant on reward ($p=0.64$). In the Full Model, SVO significantly predicted reward and inequality reduction.

\begin{table}[h]
\centering
\caption{Experiment Results Summary (100-Agent Scale, 70 runs). Statistical significance tested via LMM.}
\label{tab:results}
\begin{tabular}{lcccc}
\toprule
Hypothesis & Metric & Coeff (SE) & p-value & Cohen's $f^2$ \\
\midrule
H1 & Reward & $-0.011$ $(0.001)$ & $\mathbf{0.0003}$ & $0.40$ (Large) \\
H2 & Cooperation & $-0.003$ $(0.001)$ & $\mathbf{<0.0001}$ & $0.17$ (Medium) \\
H3 & Gini (Fairness) & $+0.078$ $(0.003)$ & $\mathbf{<0.0001}$ & $10.2$ (Huge) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Scalability and Robustness}
Scaling from 20 to 100 agents revealed structural robustness. While reward patterns remained consistent (Fig. \ref{fig:scale}a), the reduction in inequality became super-linearly stronger at scale ($f^2: 5.8 \to 10.2$).

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\linewidth]{figures/fig10_scale_comparison.png} % Crop needed
        \caption{Reward Consistency}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\linewidth]{figures/fig9_role_specialization.png}
        \caption{Role Specialization}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\linewidth]{figures/fig6_causal_forest.png}
        \caption{Causal Effect (ATE)}
    \end{subfigure}
    \caption{(a) Effect of SVO on reward is consistent across scales. (b) Divergence in cleaning thresholds indicates role specialization. (c) ATE showing meta-ranking's causal impact.}
    \label{fig:scale}
\end{figure}

\subsection{Emergent Role Specialization}
The counter-intuitive negative effect on cooperation rate (H2) results from \textit{division of labor}. Altruistic groups maintain higher diversity in cleaning thresholds $\sigma$ (0.08 vs 0.06), implying a specialized minority of ``Cleaners'' supports the population.

\subsection{Baseline Ablation: Meta-Ranking is the Key Mediator}
Direct comparison between the Full Model and Baseline (Meta-Ranking OFF) at 100-agent scale (Fig.~\ref{fig:baseline}) confirms that meta-ranking is the causal mechanism:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/fig11_baseline.png}
    \caption{Baseline (gray) vs Full Model (blue) comparison across key metrics.}
    \label{fig:baseline}
\end{figure}

\subsection{Human-AI Behavioral Alignment}
We validated agent behaviors against human Public Goods Game (PGG) data using Wasserstein Distance (WD):
\begin{itemize}
    \item Cooperation Rate: $WD = 0.178$
    \item Inequality (Gini): $WD = 0.153$
\end{itemize}
The low divergence ($WD < 0.2$) suggests that ``Situational Commitment'' mirrors human ``Conditional Cooperation.''

\section{Robustness Analysis}

\subsection{Convergence Verification}
Augmented Dickey-Fuller (ADF) tests confirmed stationarity ($p < 0.05$) in the convergence zone (last 30\% of training), with 87\% (61/70) of runs showing converged learning curves (slope $\approx 0$). See Figure~\ref{fig:convergence}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/fig13_convergence.png}
    \caption{Learning curves with convergence zone analysis. ADF stationarity confirmed across all 7 SVO conditions.}
    \label{fig:convergence}
\end{figure}

\subsection{Risk-Adjusted Comparison: Dynamic vs Static $\lambda$}
While Static $\lambda$ achieves higher raw effect sizes ($f^2 = 2.53$ vs $1.50$), Dynamic $\lambda$ provides \textbf{significantly lower variance} (Levene's test $p < 0.0001$; CV: $0.109$ vs $0.161$). This confirms Dynamic $\lambda$ as the \textbf{risk-adjusted superior strategy} (Table~\ref{tab:risk}).

\begin{table}[h]
\centering
\caption{Risk-adjusted performance comparison (Reward metric).}
\label{tab:risk}
\begin{tabular}{lcccc}
\toprule
Model & Mean & Std & CV & Sharpe \\
\midrule
Dynamic $\lambda$ & $-0.132$ & $\mathbf{0.014}$ & $\mathbf{0.109}$ & $-9.17$ \\
Static $\lambda$ & $-0.147$ & $0.024$ & $0.161$ & $-6.19$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Sensitivity Analysis}
Analysis across all 7 SVO conditions confirmed the meta-ranking effect is not parameter-dependent: significant effects ($p < 0.05$) were observed in 5/7 conditions for both Reward and Gini. Seed robustness (CV $< 0.1$) held in 52\% of cells, with remaining variance attributable to stochastic exploration.

\section{Conclusion}
We demonstrated that \textbf{Situational Commitment}---morality conditional on survival---is the only evolutionarily stable strategy in resource-constrained MARL. Our agents' behavioral distribution closely mirrors human conditional cooperation ($WD < 0.2$), suggesting that meta-ranking captures a fundamental dynamic of intelligent social behavior. This suggests AI alignment should focus on learning \emph{when} to be moral, rather than hard-coding static values.

\bibliography{references}
\bibliographystyle{plainnat}

\end{document}
