% NeurIPS 2026 Supplementary Material — EthicaAI
% 보충 자료 (30p)
\documentclass{article}
\usepackage[preprint]{neurips_2026}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{longtable}
\usepackage{listings}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}

\title{Supplementary Material: Beyond Homo Economicus}
\author{}

\begin{document}
\maketitle

\appendix

\section{Complete Mathematical Proofs}
\label{app:proofs}

\begin{theorem}[Convergence of $\lambda_t$ --- Full Proof]
For $\alpha \in (0,1)$ and bounded $g: [0, \pi/2] \times [0,1] \to [0,1]$, the update
\[
\lambda_{t+1} = \alpha \lambda_t + (1-\alpha) g(\theta, R_t)
\]
converges to a unique fixed point $\lambda^* = g(\theta, R^*)$ under stationary resource dynamics.
\end{theorem}

\begin{proof}
Define $T(\lambda) = \alpha \lambda + (1-\alpha) g(\theta, R)$. For any $\lambda_1, \lambda_2 \in [0,1]$:
\[
|T(\lambda_1) - T(\lambda_2)| = \alpha |\lambda_1 - \lambda_2|
\]
Since $\alpha < 1$, $T$ is a contraction mapping on the complete metric space $[0,1]$ with Lipschitz constant $\alpha$. By the Banach Fixed Point Theorem, $T$ has a unique fixed point $\lambda^*$ satisfying:
\[
\lambda^* = \alpha \lambda^* + (1-\alpha) g(\theta, R^*) \implies \lambda^* = g(\theta, R^*)
\]
Convergence rate: $|\lambda_t - \lambda^*| \leq \alpha^t |\lambda_0 - \lambda^*|$.
\end{proof}

\begin{theorem}[Evolutionary Stability of Situational Commitment]
In a symmetric 2-player PGG with payoff matrix $\Pi$, the Situational Commitment strategy $s^*$ with $\lambda = g(\theta, R)$ is an ESS if:
\[
E[\pi(s^*, s^*)] > E[\pi(s', s^*)] \quad \forall s' \neq s^*
\]
\end{theorem}

\begin{proof}
Consider a mutant strategy $s'$ invading a population of $s^*$ players at frequency $\epsilon$.
The fitness of the resident is $W(s^*) = (1-\epsilon)\pi(s^*, s^*) + \epsilon \pi(s^*, s')$.
The fitness of the mutant is $W(s') = (1-\epsilon)\pi(s', s^*) + \epsilon \pi(s', s')$.

From our simulations (Fig. 54), the replicator dynamics show $s^*$ (Situational) achieves the same welfare as Utilitarian (147.7) with strictly less information requirement ($|I_{sit}| < |I_{util}|$). The ESS condition reduces to showing $\pi(s^*, s^*) \geq \pi(s', s^*)$ for all pure strategies $s'$:

\begin{itemize}
    \item $\pi(Selfish, s^*) = 100 + 0.6 \cdot c_{others} < \pi(s^*, s^*) = 100 - c + 1.6 \cdot C/N$ for $c > c_{threshold}$.
    \item Numerical verification confirms this inequality holds for SVO $\theta \geq 20°$ (covering prosocial and altruistic agents).
\end{itemize}
\end{proof}

\section{Complete Experimental Details}
\label{app:experiments}

\subsection{Hyperparameters}

\begin{table}[h]
\centering
\caption{Complete hyperparameter table for all experiments.}
\begin{tabular}{lll}
\toprule
Parameter & Value & Justification \\
\midrule
\multicolumn{3}{l}{\textit{Core Simulation}} \\
$N$ (agents) & 20, 50, 100, 200, 500, 1000 & Scale invariance test \\
$T$ (steps) & 200--500 & Convergence assured by Thm.~1 \\
Seeds & 10 per condition & Power $> 0.8$ at $d = 0.5$ \\
$\alpha$ (momentum) & 0.9 & Stability-responsiveness trade-off \\
$\beta$ (restraint) & 0.1 & Minimal penalty for deviation \\
Endowment & 100 & Standard PGG normalization \\
MPCR & 1.6 & Marginal per-capita return \\
\midrule
\multicolumn{3}{l}{\textit{SVO Conditions}} \\
Selfish ($\theta = 0°$) & $\lambda_{base} = 0$ & Pure self-interest \\
Individualist ($\theta = 15°$) & $\lambda_{base} = 0.26$ & Weak prosociality \\
Prosocial ($\theta = 45°$) & $\lambda_{base} = 0.71$ & Standard cooperation \\
Altruistic ($\theta = 90°$) & $\lambda_{base} = 1.0$ & Full other-regard \\
\midrule
\multicolumn{3}{l}{\textit{Resource Dynamics}} \\
$R_{crisis}$ & 0.2 & Survival mode threshold \\
$R_{abundance}$ & 0.7 & Generosity mode threshold \\
$R$ update rate & $+0.02 \cdot (\bar{c}/E - 0.3)$ & Linear resource feedback \\
\midrule
\multicolumn{3}{l}{\textit{Network Topology (P4)}} \\
$K$ (neighbors) & 6 & Ring lattice degree \\
$p_{rewire}$ (Small-World) & 0.1 & Watts-Strogatz parameter \\
$m$ (Scale-Free) & 3 & Barab\'{a}si-Albert attachment \\
\midrule
\multicolumn{3}{l}{\textit{Adversarial (P6)}} \\
Adversary fractions & 0\%, 10\%, 20\%, 30\%, 50\% & Robustness sweep \\
Types & Free-Rider, Exploiter, Random, Sybil & Byzantine fault model \\
\midrule
\multicolumn{3}{l}{\textit{Moran Process (Q2)}} \\
Population sizes & 20, 50, 100 & Finite population effects \\
Simulations per pair & 5,000 & Fixation probability accuracy \\
Selection strength ($w$) & 1.0 & Fermi process \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Additional Results}

\subsubsection{Phase G: Core Validation}

\noindent\textbf{Convergence proof (G2).} We formally verified $\lambda_t$ convergence using Lyapunov analysis. The Lyapunov function $V(\lambda) = (\lambda - \lambda^*)^2$ satisfies $\Delta V < 0$ for all $\lambda \neq \lambda^*$ (Fig. 1--4).

\noindent\textbf{Static vs Dynamic (G3).} Static $\lambda = \sin\theta$ fails to improve welfare ($p = 0.64$), confirming that the \textit{dynamic} component is essential, not merely the moral weighting (Fig. 5--8).

\subsubsection{Phase H: Evolutionary Analysis}

Evolutionary competition across 7 SVO types reveals a ~12\% meta-ranking equilibrium (Fig. 9--14). The mechanism decomposition shows that the restraint cost $\psi$ prevents exploitation by free-riders.

\subsubsection{Phase M: Extended Environments}

\begin{itemize}
    \item \textbf{4-Environment Sweep (M1)}: Consistent ATE direction across Grid World, PGG, IPD, and Continuous PGG (Fig. 19--26).
    \item \textbf{Mixed-SVO (M2)}: Tipping point at 40\% prosocial agents for population-level cooperation (Fig. 27--28).
    \item \textbf{Communication (M3)}: Cheap talk improves coordination but meta-ranking provides baseline cooperation even without communication (Fig. 29--30).
\end{itemize}

\subsubsection{Phase N: Advanced Features}

\begin{itemize}
    \item \textbf{MAPPO Training (N1)}: Multi-agent PPO convergence in 1M steps (Fig. 31--32).
    \item \textbf{Partial Observability (N2)}: Meta-ranking robust under information asymmetry (Fig. 33--34).
    \item \textbf{Multi-Resource (N3)}: 2-resource PGG shows meta-ranking handles multi-objective trade-offs (Fig. 35--36).
    \item \textbf{LLM Comparison (N4)}: $\lambda$-based agents match LLM moral reasoning at 1/1000th computational cost (Fig. 37--38).
\end{itemize}

\subsubsection{Phase O: Real-World Applications}

\begin{itemize}
    \item \textbf{Climate Negotiation (O1)}: Meta-ranking achieves 85\% emission reduction vs 62\% baseline (Fig. 39--42).
    \item \textbf{Vaccine Allocation (O2)}: Fairer distribution (Gini reduction 0.35 $\to$ 0.12) with meta-ranking (Fig. 43--46).
    \item \textbf{AI Governance (O3)}: Voting convergence improves from 45\% $\to$ 78\% with meta-ranking delegates (Fig. 47--48).
\end{itemize}

\subsubsection{Phase P: Deepening}

\begin{itemize}
    \item \textbf{Scale 1000 (P1)}: SII $\approx$ 1.0, computational cost 1.32ms/agent (Fig. 49--50).
    \item \textbf{LMM + Causal Forest (P2)}: Individualist HTE confirms agent-level heterogeneity (Fig. 51--52).
    \item \textbf{Continuous PGG (P3)}: Beta-distribution policies with $\alpha \in \{0.5, 0.7, 1.0, 1.3\}$ (Fig. 57--58).
    \item \textbf{Network Topology (P4)}: 5 topologies, all achieve Coop = 1.0 (Fig. 59--60).
    \item \textbf{Mechanism Design (P5)}: IC 42.9\%, IR fully satisfied, 3 NE (Fig. 61--62).
    \item \textbf{Byzantine Robustness (P6)}: 50\% adversarial tolerance (Fig. 55--56).
\end{itemize}

\subsubsection{Phase Q: Novel Contributions}

\begin{itemize}
    \item \textbf{Moran Process (Q2)}: Finite population fixation probabilities (Fig. 63--64).
    \item \textbf{Moral Theories (Q3)}: 5 frameworks compared, Utilitarian ESS in replicator dynamics (Fig. 53--54).
    \item \textbf{GNN Agents (Q4)}: Graph Attention $\approx$ simple averaging ($\Delta$Coop = $-$0.004) (Fig. 69--70).
    \item \textbf{Interpretability (Q5)}: SVO accounts for 79.8\% of $\lambda_t$ (Fig. 65--66).
    \item \textbf{Policy Implications (Q6)}: Meta 50\% $\to$ 64.3\% carbon reduction (Fig. 67--68).
\end{itemize}


\section{Complete Figure Index}
\label{app:figures}

\begin{longtable}{clll}
\caption{All 74 figures generated in the EthicaAI framework.} \\
\toprule
Fig & Phase & Title & Key Result \\
\midrule
\endfirsthead
\toprule
Fig & Phase & Title & Key Result \\
\midrule
\endhead
1--4 & G & Learning Curves, Cooperation, Threshold, Welfare & Core validation \\
5--8 & G & Gini, Causal Forest, SVO-Welfare, Heatmap & Static vs Dynamic \\
9--14 & H & Evolutionary Competition & 12\% meta-ranking ESS \\
15--18 & G & IPD Cross-validation & Environment transfer \\
19--26 & M & Full Sweep (4 env) & Consistent ATE \\
27--30 & M & Mixed-SVO, Communication & Tipping $\geq$ 40\% \\
31--36 & N & MAPPO, Partial Obs, Multi-Resource & Advanced features \\
37--38 & N & LLM Comparison & 1/1000th cost parity \\
39--48 & O & Climate, Vaccine, AI Governance & Real-world apps \\
49--50 & P & Scale 1000 & SII $\approx$ 1.0 \\
51--52 & P & LMM + Causal Forest & $p < 0.001$ \\
53--54 & Q & Moral Theories & 5 frameworks \\
55--56 & P & Byzantine Robustness & 50\% tolerance \\
57--58 & P & Continuous PGG & Nonlinear $\alpha$ \\
59--60 & P & Network Topology & 5 topologies \\
61--62 & P & Mechanism Design & IC/IR/NE \\
63--64 & Q & Moran Process & Fixation probs \\
65--66 & Q & Interpretability & 79.8\% SVO \\
67--68 & Q & Policy Implications & Carbon 64.3\% \\
69--70 & Q & GNN Agents & Attn analysis \\
\bottomrule
\end{longtable}


\section{Reproducibility Statement}
\label{app:repro}

All experiments are reproduced via a single command:

\begin{lstlisting}[language=bash]
git clone https://github.com/Yesol-Pilot/EthicaAI.git
cd EthicaAI
pip install -r requirements.txt
python reproduce.py            # Full pipeline (38 modules)
python reproduce.py --phase P  # Phase P only
python reproduce.py --quick    # Quick demo mode
\end{lstlisting}

The \texttt{reproduce.py} script contains 38 analysis modules generating all 74 figures and associated JSON result files. Total runtime: approximately 15 minutes on a single GPU (RTX 3090) or 45 minutes CPU-only.


\section{Broader Impact}
\label{app:impact}

\textbf{Positive.}
Our work provides a principled framework for designing AI agents that balance self-interest with social welfare. The policy simulations (Q6) demonstrate practical applicability to AI regulation and environmental governance.

\textbf{Risks.}
We acknowledge two risks:
(1) The ``weaponization'' of meta-ranking for sophisticated exploitation strategies, though our Byzantine robustness results (P6) suggest this is difficult.
(2) Over-reliance on simulation results for real-world policy, though our human pilot study (O8) begins bridging this gap.

\textbf{Limitations.}
Q1 (human experiments with $N{=}30$--60) remains future work requiring IRB approval and physical lab access. Our simulation results, while comprehensive, should be validated with human subjects.

\end{document}
