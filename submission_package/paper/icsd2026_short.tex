% ICSD 2026 Short Paper (6 pages) — EthicaAI
% International Conference on Social Dilemmas, June 2026, Japan
\documentclass[12pt]{article}
\usepackage[a4paper, margin=2.5cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{natbib}

\title{When Should AI Agents Be Moral?\\
Computational Verification of Sen's Meta-Ranking in Social Dilemmas}

\author{Yesol Heo\\
Independent Researcher, Seoul\\
\texttt{dpfh1537@gmail.com}}

\date{ICSD 2026 — June, Japan}

\begin{document}
\maketitle

\begin{abstract}
We formalize Amartya Sen's Meta-Ranking theory within a Multi-Agent Reinforcement Learning framework, demonstrating that dynamic moral commitment ($\lambda_t$)---conditional on resource availability---significantly outperforms static value injection across 8 environments with up to 1,000 agents. Our key finding is that \textit{Situational Commitment}---morality conditional on survival---is the only Evolutionarily Stable Strategy, achieving utilitarian-optimal welfare using only local information. We demonstrate scale invariance (SII$\approx$1.0), Byzantine robustness (50\% adversarial tolerance), and policy implications for environmental governance.
\end{abstract}

\section{Introduction}
Sen's \cite{sen1977rational} critique of Rational Choice Theory introduced ``meta-rankings''---preferences over preferences---as the mechanism for genuine moral commitment. We implement this computationally via a dynamic $\lambda_t$ parameter that modulates the weight between self-interest and social welfare based on resource availability and Social Value Orientation (SVO).

\section{Framework}
Agent $i$'s reward combines self-interest and meta-preference:
$R_{\text{total}}^{(i)} = (1-\lambda_t) \cdot U_{\text{self}} + \lambda_t \cdot [U_{\text{meta}} - \psi]$
where $\lambda_t$ adapts dynamically via an exponential moving average conditioned on resource level.

\section{Key Results}
\begin{itemize}
    \item Dynamic meta-ranking enhances welfare ($p<0.001$ via LMM), while static injection fails ($p=0.64$)
    \item Situational Commitment outperforms 4 other moral frameworks (Utilitarian, Deontological, Virtue, Selfish)
    \item Scale invariant from 20 to 1,000 agents (SII$\approx$1.0, 1.32ms/agent)
    \item Byzantine robust at 50\% adversarial population (Coop=1.000 maintained)
    \item Policy simulation: 50\% meta-ranking adoption reduces carbon emissions by 64.3\%
\end{itemize}

\section{Discussion}
Our results validate Sen's insight computationally: genuine commitment requires awareness of survival constraints. This has implications for both AI alignment and environmental governance, suggesting that internal moral mechanisms can substitute for external regulatory pressure.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
